# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AbHgTJJ2EZY2o4d42GLbGFVtspNk1OgO
"""

!pip install jiwer
!pip install tqdm
!pip install numpy
!pip install os
!pip install wandb
!pip install datasets
!pip install torch
!pip install torch-xla
import torch
import torch_xla
import torch_xla.core.xla_model as xm
import torch_xla.distributed.parallel_loader as pl
import torchaudio
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler  # For GPU
from transformers import (
    WhisperForConditionalGeneration,
    WhisperFeatureExtractor,
    WhisperProcessor,
    PreTrainedTokenizerFast,
    get_scheduler,
)
# from peft import LoraConfig, get_peft_model  # Uncomment for GPU LoRA
from tokenizers import ByteLevelBPETokenizer
from datasets import load_dataset, Audio
from jiwer import wer
from tqdm import tqdm
import numpy as np
import os
import wandb

#Device setup (GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
if device.type == "cuda":
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

from huggingface_hub import notebook_login
notebook_login()

# Load dataset (315 rows)
dataset = load_dataset("mozilla-foundation/common_voice_17_0", "sat", split="train")
test_dataset = load_dataset("mozilla-foundation/common_voice_17_0", "sat", split="test")
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
test_dataset = test_dataset.cast_column("audio", Audio(sampling_rate=16000))
print(f"Training dataset size: {len(dataset)} rows")

# Load the trained tokenizer (update with correct paths)
custom_tokenizer = ByteLevelBPETokenizer(
    "/content/vocab.json",  # Path to your tokenizer files
    "/content/merges.txt"
)

# Convert tokenizer to Hugging Face format
tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=custom_tokenizer._tokenizer, # Access the underlying tokenizer object
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    mask_token="<mask>",
  )

def get_max_length(dataset, tokenizer, text_column="sentence"):
    max_length = 0
    for example in dataset:
        tokens = tokenizer(example[text_column], truncation=False)["input_ids"]
        max_length = max(max_length, len(tokens))
    return max_length

max_token_length = get_max_length(dataset, tokenizer)
print(f"Maximum token length: {max_token_length}")

feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-tiny")

def preprocess_function(batch, tokenizer, max_audio_length=10*16000):
    audio_arrays = [x["array"] for x in batch["audio"]]
    sampling_rates = [x["sampling_rate"] for x in batch["audio"]]
    transcriptions = batch["sentence"]

    resampler = torchaudio.transforms.Resample(orig_freq=sampling_rates[0], new_freq=16000)
    audio_arrays = [resampler(torch.tensor(audio)).numpy() for audio in audio_arrays]

    audio_arrays = [
        audio[:max_audio_length] if len(audio) > max_audio_length else
        np.pad(audio, (0, max_audio_length - len(audio)), mode="constant")
        for audio in audio_arrays
    ]

    inputs = feature_extractor(
        audio_arrays, sampling_rate=16000, return_tensors="pt", padding="max_length", max_length=max_audio_length
    )

    # Pad mel spectrogram to 3000 time steps (Whisperâ€™s expected length)
    mel_features = inputs["input_features"]  # Shape: (batch_size, 80, 1000)
    batch_size, n_mels, seq_len = mel_features.shape
    if seq_len < 3000:
        padding = torch.zeros(batch_size, n_mels, 3000 - seq_len, dtype=mel_features.dtype)
        inputs["input_features"] = torch.cat([mel_features, padding], dim=-1)  # Shape: (batch_size, 80, 3000)

    labels = tokenizer(
        transcriptions,
        max_length=max_token_length,
        padding="max_length",
        truncation=True,
        return_tensors="pt",
    ).input_ids

    labels = labels.masked_fill(labels == tokenizer.pad_token_id, -100)
    inputs["labels"] = labels
    return inputs

def map_preprocessing(dataset, tokenizer):
    return dataset.map(
        lambda batch: preprocess_function(batch, tokenizer),
        batched=True,
        batch_size=16,
        remove_columns=dataset.column_names,
    )

processed_dataset = map_preprocessing(dataset, tokenizer)
processed_dataset.set_format(type="torch", columns=["input_features", "labels"])

def custom_collate_fn(batch):
    input_features = torch.stack([item["input_features"] for item in batch])
    labels = torch.stack([item["labels"] for item in batch])
    return {"input_features": input_features, "labels": labels}

# Batch size and accumulation for GPU
batch_size = 4
accumulation_steps = 4  # Effective batch size = 16
train_dataloader = DataLoader(
    processed_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=0,
    pin_memory=True if device.type == "cuda" else False,
    collate_fn=custom_collate_fn,
)

# Load Whisper Tiny with optimizations
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-tiny").to(device)
model.config.forced_decoder_ids = None

# Enable gradient checkpointing
model.gradient_checkpointing_enable()

# Install the peft library if you haven't already
!pip install peft

# Import the necessary classes
from peft import LoraConfig, get_peft_model

# Enable LoRA
lora_config = LoraConfig(
    r=32,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "out_proj"],
    lora_dropout=0.1,
    bias="none",
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # ~1-2M parameters

# Training setup
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
scaler = GradScaler()
num_training_steps = len(train_dataloader) * 20
lr_scheduler = get_scheduler(
    "cosine", optimizer=optimizer, num_warmup_steps=100, num_training_steps=num_training_steps
)

wandb.init(project="whisper-santali-asr", name="gpu-tiny-lora-finetuning")
max_epochs = 2
running_loss = []

for epoch in range(max_epochs):
    model.train()
    print(f"Epoch {epoch+1}/{max_epochs}")
    for step, batch in enumerate(tqdm(train_dataloader, desc="Training")):
        input_features = batch["input_features"].to(device)
        labels = batch["labels"].to(device)

        with autocast():
            outputs = model(input_features, labels=labels)
            loss = outputs.loss / accumulation_steps

        scaler.scale(loss).backward()
        if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_dataloader):
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            lr_scheduler.step()

        running_loss.append(loss.item() * accumulation_steps)
        wandb.log({"loss": running_loss[-1], "learning_rate": optimizer.param_groups[0]["lr"]})

        if step == 0 or step % 20 == 0:
            print(f"Step {step}, Loss: {running_loss[-1]}")
            if device.type == "cuda":
                print(f"GPU Memory Allocated: {torch.cuda.memory_allocated(device) / 1e9:.2f} GB")

# Save final model
torch.save(model.state_dict(), "whisper_santali_tiny_lora.pt")
tokenizer.save_pretrained("whisper_santali_tiny_lora")
feature_extractor.save_pretrained("whisper_santali_tiny_lora")

# Evaluation
processor = WhisperProcessor.from_pretrained("openai/whisper-tiny")
model.eval()
def evaluate_model(model, dataset, processor, tokenizer):
    predictions, references = [], []

    for example in tqdm(dataset, desc="Evaluating"):
        try:
            audio_array = example["audio"]["array"]
            if not audio_array.size or not example["sentence"]:
                predictions.append("")
                references.append(example["sentence"])
                continue

            # Preprocess single audio sample and pad to 3000 time steps
            inputs = processor(audio_array, sampling_rate=16000, return_tensors="pt")
            mel_features = inputs["input_features"]  # Shape: (1, 80, 1000)
            if mel_features.shape[-1] < 3000:
                padding = torch.zeros(1, 80, 3000 - mel_features.shape[-1], dtype=mel_features.dtype)
                inputs["input_features"] = torch.cat([mel_features, padding], dim=-1)

            inputs["input_features"] = inputs["input_features"].to(device)
            with torch.no_grad():
                predicted_ids = model.generate(inputs["input_features"])
            predicted_text = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]
            predictions.append(predicted_text)
            references.append(example["sentence"])
        except Exception as e:
            print(f"Error processing example: {e}")
            predictions.append("")
            references.append(example["sentence"])

    wer_score = wer(references, predictions)
    return wer_score

wer_score = evaluate_model(model, test_dataset, processor, tokenizer)
print(f"Final WER: {wer_score:.4f}")

print("Training and Evaluation Complete! ðŸš€")
wandb.finish()

wer_score